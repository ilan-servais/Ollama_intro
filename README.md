# Chat Ollama Local avec Llama 3.2

Ce projet est une interface web simple pour interagir avec le mod√®le Llama 3.2 via Ollama en local sur votre ordinateur.

## Pr√©requis

- Un ordinateur avec Windows, macOS ou Linux
- Au moins 8 Go de RAM (16 Go recommand√©s)
- Environ 9 Go d'espace disque pour le mod√®le Llama 3.2
- Navigateur web moderne (Chrome, Firefox, Edge, Safari)

## Installation d'Ollama

1. T√©l√©chargez Ollama depuis le site officiel: [ollama.com](https://ollama.com)
2. Installez le logiciel en suivant les instructions pour votre syst√®me d'exploitation:
   - **Windows**: Ex√©cutez l'installateur et suivez les instructions
   - **macOS**: Glissez l'application dans votre dossier Applications
   - **Linux**: Utilisez la commande fournie sur le site

## T√©l√©chargement du mod√®le Llama 3.2

1. Ouvrez un terminal (ou invite de commandes sous Windows)
2. Ex√©cutez la commande suivante:

```bash
ollama pull llama3.2
```
ou
```bash
ollama pull llama3.2:latest
```

3. Attendez que le t√©l√©chargement soit termin√© (environ 9 Go)

## Utilisation de l'interface web

1. Assurez-vous qu'Ollama est d√©marr√© et fonctionne en arri√®re-plan
2. Ouvrez le fichier `index.html` dans votre navigateur
3. Tapez votre message dans la zone de texte
4. Cliquez sur "Envoyer"
5. Attendez la r√©ponse du mod√®le

## Personnalisation

Vous pouvez modifier le mod√®le utilis√© en changeant la ligne suivante dans le fichier `index.html`:

```javascript
body: JSON.stringify({ model: 'llama3.2:latest', prompt: prompt })
```

Remplacez `llama3.2:latest` par un autre mod√®le disponible sur Ollama.

## Commandes utiles d'Ollama

- Liste des mod√®les install√©s: `ollama list`
- Supprimer un mod√®le: `ollama rm [nom-du-mod√®le]`
- Ex√©cuter un mod√®le directement dans le terminal: `ollama run [nom-du-mod√®le]`

## Bien fermer Ollama correctement

Sur Windows, Ollama peut se lancer automatiquement en t√¢che de fond. Voici comment le fermer compl√®tement :

1. Cliquez sur la fl√®che ^ dans la barre des t√¢ches (en bas √† droite de l‚Äô√©cran)
2. Faites un clic droit sur l'ic√¥ne Ollama
3. Cliquez sur **Quitter**
4. V√©rifiez que le serveur est bien arr√™t√© en allant sur `http://localhost:11434` ‚Äî s‚Äôil ne r√©pond plus, Ollama est bien ferm√©
5. Optionnel : lancez la commande suivante pour v√©rifier que plus aucun mod√®le ne tourne :
   ```bash
   ollama ps
   ```
   Si rien n‚Äôest affich√©, tout est stopp√©.

üí° Note : Le simple fait d‚Äôex√©cuter une commande `ollama` dans le terminal peut red√©marrer automatiquement le service. Pensez √† bien le quitter si vous ne voulez pas qu‚Äôil tourne.

## D√©pannage

- Si vous rencontrez l'erreur "Connection refused", v√©rifiez qu'Ollama est bien d√©marr√©
- Si le mod√®le est lent, v√©rifiez que votre ordinateur a assez de ressources disponibles
- Sur Windows, assurez-vous que le pare-feu ne bloque pas Ollama

## Ressources suppl√©mentaires

- [Documentation Ollama](https://github.com/ollama/ollama/blob/main/README.md)
- [Documentation Llama 3](https://ai.meta.com/llama/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/)

